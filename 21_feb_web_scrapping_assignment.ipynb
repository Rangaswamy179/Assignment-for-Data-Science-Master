{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc870b8-5831-4253-85a0-af90d9ac1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ea359-83af-4d56-a1ce-3f19c5ca9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "Web scraping is the process of extract content and data from a website.\n",
    "it is mainly used to analyse the existing data for later purpose\n",
    "three areas where Web Scraping is used to get data.:\n",
    "    Crawling and indexing websites for search engines.\n",
    "    Collecting data for market research or competitor analysis.\n",
    "    Populating news feeds.\n",
    "    Extracting data to train machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bd9e5-3247-422b-8603-e4e0379386d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724f255-f424-49d9-8757-2f1df2198bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "The most common techniques used for Web Scraping are\n",
    "\n",
    "Human copy-and-paste.\n",
    "Text pattern matching.\n",
    "HTTP programming.\n",
    "HTML parsing.\n",
    "DOM parsing.\n",
    "Vertical aggregation.\n",
    "Semantic annotation recognizing.\n",
    "Computer vision web-page analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e1e75-58d0-4a62-ae2e-78da043f0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a653150-256d-41ec-8a44-dfb1a68707c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beautiful Soup is a Python library that makes it easy to scrape information from \n",
    "web pages. It sits atop an HTML or XML parser and provides Pythonic idioms for \n",
    "iterating, searching, and modifying the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8ec43-530e-4eb5-8447-c33a63187a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Beautiful Soup library helps with isolating titles and links from webpages. \n",
    "It can extract all of the text from ​HTML tags, and alter the HTML \n",
    "​in the document with which we’re working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fc440-7a20-4563-8ec9-b8972d949ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98e934-98b5-4f9c-8e1d-3e269c6958da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data \n",
    "and display it as HTML in a new HTML file. The requests module allows us to send http requests \n",
    "to the website we want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833da270-092b-450e-a741-d70a157ed77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164a21d-f253-4e5d-81d2-7de01a257645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic Bean Stack and code pipelines are the names of AWS services used in this project\n",
    "Elastic Bean Stack mainly used for the deployment of an application in cloud it provides whole\n",
    "machine that as application runs like cpu,RAM,storage systems,etc where application runs in a\n",
    "server and we where ever we can use the application\n",
    "code pipline fetch data from the github where our application it act as intermediate between \n",
    "bean stack and github and create pipe lines through app data is transfered and by which we\n",
    "can deploy the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
