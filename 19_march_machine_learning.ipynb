{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf2df3-1fbe-445f-ad8e-dd005d61f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776faa5e-78a1-489a-bbe5-b2b7689d0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "min-max scaling defined as we scale and normalized the data that ranges between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b477ac-bd86-4266-9ef6-36a9faa0e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling is useful in data preprocessing for several reasons:\n",
    "\n",
    "It ensures that all features are on a comparable scale, preventing one feature from dominating others during analysis or model training.\n",
    "It helps algorithms that rely on distance calculations or assume a Gaussian distribution of features to work properly.\n",
    "It can improve the convergence speed and performance of certain optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c21798-2532-4cb6-b800-35905c51e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db01424c-ebb5-47c7-8d77-7aa825524d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7e3018-c905-4dd0-ae52-5fbd8067c478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27117366, 0.01415106],\n",
       "       [0.4722292 , 0.13913574],\n",
       "       [0.32143755, 0.01546857],\n",
       "       ...,\n",
       "       [       nan, 0.04577135],\n",
       "       [0.32143755, 0.0585561 ],\n",
       "       [0.39683338, 0.01512699]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "min_max.fit_transform(df[['age','fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b3447-f60c-4d9a-85ca-607ca32ed321",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268d314-a927-40d7-8641-91cc59bdb424",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique, also known as vector normalization or feature scaling by vector magnitude, is a data preprocessing method used to scale features to unit length.\n",
    "In this technique, each feature vector is divided by its magnitude or Euclidean norm to transform it into a unit vector.\n",
    "formula for unit vector:\n",
    "    unit_vector=x/||x||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e3801-42c7-43d3-8268-07d1dda0717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique differs from Min-Max scaling in the sense that it focuses on the direction of the feature vectors \n",
    "rather than their values. By normalizing the vectors, the relative orientations\n",
    "of the vectors are preserved while their lengths are adjusted to be unit length.\n",
    "\n",
    "Unit Vector scaling is useful when the magnitude of a feature vector is not as\n",
    "important as its direction. It is commonly used in machine learning algorithms\n",
    "that rely on the angle or orientation between feature vectors, such as cosine \n",
    "similarity or clustering algorithms like K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0a4e9-8c4d-4659-9a63-c90116c3bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "suppose dataset contains features 3 and 4,we have to scale the featues by unit vector.we first calculate\n",
    "the unit vector magnitude sqrt(3^2+4^2) is 5 and we divide each feature by magnitude of unit vector and we scale\n",
    "the value to(3/5,4/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250a806-52cd-40d5-ab05-864ac99f8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae9562-6339-4e0c-a71a-e31050d25e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation while retaining the most important information. It identifies the principal components, which are new variables that are linear combinations of the original features. These principal components are ordered in terms of the amount of variance they explain in the data.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "Standardize the data: If the features in the dataset have different scales, it is important to standardize them (subtracting the mean and dividing by the standard deviation) to ensure that all features contribute equally to the analysis.\n",
    "\n",
    "Compute the covariance matrix: The covariance matrix is calculated based on the standardized features. It represents the relationships and dependencies between the features.\n",
    "\n",
    "Calculate the eigenvectors and eigenvalues: The eigenvectors represent the directions or axes of the principal components, while the eigenvalues indicate the amount of variance explained by each principal component. The eigenvectors and eigenvalues are derived from the covariance matrix.\n",
    "\n",
    "Select the principal components: The principal components are selected based on their corresponding eigenvalues. The components with higher eigenvalues explain more variance in the data and are considered more important.\n",
    "\n",
    "Transform the data: The original dataset is transformed into the new lower-dimensional space by projecting it onto the selected principal components.\n",
    "\n",
    "PCA is commonly used in dimensionality reduction for various purposes, such as data visualization, noise reduction, and feature extraction. By reducing the dimensionality of the dataset, PCA helps in simplifying the analysis, improving computational efficiency, and eliminating redundant or irrelevant features.\n",
    "\n",
    "Here's an example to illustrate the application of PCA for dimensionality reduction:\n",
    "\n",
    "Suppose you have a dataset of housing features with the following five numeric variables:\n",
    "\n",
    "Square footage\n",
    "Number of bedrooms\n",
    "Number of bathrooms\n",
    "Distance to the nearest school\n",
    "Distance to the city center\n",
    "You want to reduce the dimensionality of the dataset while retaining as much information as possible.\n",
    "\n",
    "First, you standardize the data by subtracting the mean and dividing by the standard deviation for each variable. This ensures that all variables are on a similar scale.\n",
    "\n",
    "Next, you compute the covariance matrix based on the standardized data. The covariance matrix indicates the relationships between the variables.\n",
    "\n",
    "Then, you calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each component.\n",
    "\n",
    "Suppose the resulting eigenvalues are as follows:\n",
    "\n",
    "Eigenvalue 1: 3.0\n",
    "Eigenvalue 2: 1.5\n",
    "Eigenvalue 3: 1.2\n",
    "Eigenvalue 4: 0.8\n",
    "Eigenvalue 5: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad6b1c-6626-46ad-9b41-bcaff253b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2e0a5-da40-4e1d-8f64-2126342fba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PCA and feature extraction are closely related concepts. In fact, PCA can be used as a technique for feature extraction. Feature extraction refers to the process of transforming the original features into a new set of features that are more informative, representative, or easier to work with. PCA achieves feature extraction by creating new features called principal components, which are linear combinations of the original features.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Dataset with Original Features: Consider a dataset with multiple original features, such as age, income, education level, and occupation.\n",
    "\n",
    "Standardize the Data: To ensure that all features contribute equally, the data is standardized by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Apply PCA: PCA is then applied to the standardized data. It calculates the eigenvectors and eigenvalues of the covariance matrix, as explained earlier. The eigenvectors represent the directions or axes of the principal components.\n",
    "\n",
    "Select Principal Components: The principal components are selected based on their corresponding eigenvalues. The components with higher eigenvalues explain more variance in the data and are considered more important.\n",
    "\n",
    "Transform the Data: The original dataset is transformed into the new lower-dimensional space by projecting it onto the selected principal components. These new components are the extracted features obtained through PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da63b27-9abe-4eed-b47d-fd842b933866",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b07a95f-1aa0-45fb-8769-590624f1e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "To preprocess the dataset for building a recommendation system for a food delivery service, you can utilize Min-Max scaling. Here's how you can apply Min-Max scaling to the features of the dataset:\n",
    "\n",
    "1. Identify the features: In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "2. Calculate the minimum and maximum values: Determine the minimum and maximum values for each feature in the dataset. For example, find the minimum and maximum price, rating, and delivery time in the dataset.\n",
    "\n",
    "3. Apply Min-Max scaling formula: For each feature, apply the Min-Max scaling formula to normalize the values between 0 and 1. The formula is:\n",
    "\n",
    "   \\[X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "\n",
    "   where:\n",
    "   - \\(X\\) is the original value of the feature.\n",
    "   - \\(X_{\\text{min}}\\) is the minimum value of the feature.\n",
    "   - \\(X_{\\text{max}}\\) is the maximum value of the feature.\n",
    "   - \\(X_{\\text{scaled}}\\) is the scaled value of the feature.\n",
    "\n",
    "4. Perform Min-Max scaling: Apply the Min-Max scaling formula to each feature using the respective minimum and maximum values obtained in step 2. This will transform the values of each feature into a range between 0 and 1.\n",
    "\n",
    "For example, let's consider the following values for the features in the dataset:\n",
    "\n",
    "- Price: $5 - $30\n",
    "- Rating: 2.5 - 4.8\n",
    "- Delivery Time: 20 minutes - 60 minutes\n",
    "\n",
    "To apply Min-Max scaling, you would calculate the minimum and maximum values for each feature:\n",
    "\n",
    "Minimum price (\\(X_{\\text{min}}\\)): $5\n",
    "Maximum price (\\(X_{\\text{max}}\\)): $30\n",
    "\n",
    "Minimum rating (\\(X_{\\text{min}}\\)): 2.5\n",
    "Maximum rating (\\(X_{\\text{max}}\\)): 4.8\n",
    "\n",
    "Minimum delivery time (\\(X_{\\text{min}}\\)): 20 minutes\n",
    "Maximum delivery time (\\(X_{\\text{max}}\\)): 60 minutes\n",
    "\n",
    "Now, you can scale the values using the Min-Max scaling formula:\n",
    "\n",
    "For price:\n",
    "\\[X_{\\text{scaled}} = \\frac{\\text{price} - 5}{30 - 5}\\]\n",
    "\n",
    "For rating:\n",
    "\\[X_{\\text{scaled}} = \\frac{\\text{rating} - 2.5}{4.8 - 2.5}\\]\n",
    "\n",
    "For delivery time:\n",
    "\\[X_{\\text{scaled}} = \\frac{\\text{delivery time} - 20}{60 - 20}\\]\n",
    "\n",
    "After applying Min-Max scaling, the values for each feature will be transformed to the range between 0 and 1, making them comparable and suitable for the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c575caf-16f2-4d23-b358-c85706640a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "To reduce the dimensionality of the dataset in order to build a model for predicting stock prices, you can employ Principal Component Analysis (PCA). Here's how you can use PCA for dimensionality reduction:\n",
    "\n",
    "Prepare the dataset: Gather the dataset containing multiple features, such as company financial data (e.g., revenue, earnings, debt) and market trends (e.g., stock market index, interest rates).\n",
    "\n",
    "Standardize the data: Before applying PCA, it's important to standardize the data by subtracting the mean and dividing by the standard deviation for each feature. Standardization ensures that all features contribute equally to the PCA analysis.\n",
    "\n",
    "Apply PCA: Perform PCA on the standardized dataset. PCA computes the eigenvectors and eigenvalues of the covariance matrix of the data.\n",
    "\n",
    "Select the number of principal components: Analyze the eigenvalues obtained from PCA. The eigenvalues represent the amount of variance explained by each principal component. Sort the eigenvalues in descending order. You can then decide on the number of principal components to retain based on the cumulative explained variance or by setting a threshold (e.g., retaining components that explain a certain percentage of the total variance, such as 90%).\n",
    "\n",
    "Transform the data: Once you have determined the number of principal components, transform the original dataset by projecting it onto the selected principal components. The transformed dataset will have reduced dimensionality, with the selected principal components as the new features.\n",
    "\n",
    "Train the model: Use the transformed dataset with reduced dimensionality as input to train your stock price prediction model. You can employ various machine learning algorithms, such as regression or time series models, depending on the nature of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966b88d-a1fd-4093-b041-4b2ff8208278",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "793ad1d8-443c-4e9e-95ab-db176163e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]\n"
     ]
    }
   ],
   "source": [
    "l=[1, 5, 10, 15, 20]\n",
    "l1=[]\n",
    "mini=min(l)\n",
    "maxm=max(l)\n",
    "for i in l:\n",
    "    l1.append((i-mini)/(maxm-mini))\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896ce16-0444-4da2-abde-4c7fb4597564",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782e821-f184-482f-8d54-48cc14551829",
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of principal components to retain depends on the specific dataset and the amount of variance explained. In this case, let's assume the PCA analysis yielded the following eigenvalues:\n",
    "\n",
    "Eigenvalue 1: 2.5\n",
    "Eigenvalue 2: 1.8\n",
    "Eigenvalue 3: 1.2\n",
    "Eigenvalue 4: 0.9\n",
    "Eigenvalue 5: 0.6\n",
    "In this scenario, the cumulative explained variance may be calculated, and the eigenvalues can be normalized by dividing them by the sum of all eigenvalues. Based on the results, suppose the cumulative explained variance is around 80% after considering the first three principal components. In this case, it could be reasonable to choose to retain the first three principal components.\n",
    "\n",
    "The decision to retain a specific number of principal components depends on the trade-off between the amount of variance explained and the desired dimensionality reduction. Retaining fewer components simplifies the model but may result in some loss of information. Retaining more components can capture more details but may introduce higher dimensionality.\n",
    "\n",
    "Ultimately, the exact number of principal components to retain should be determined based on the specific requirements of the analysis, the amount of variance explained, and the trade-off between complexity and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765505bb-d35b-4d05-b54a-38a4566f44d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
