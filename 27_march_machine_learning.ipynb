{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e725d3-aff2-4c63-a843-7e5efa6efb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8860bf-9189-433a-a146-5b03a854c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared in linear regression models used for the performance matrix in model.it tells how the model is performed\n",
    "to the test data after the training of data with the model,suppose let us considerd if model peformance is 70%,it tells\n",
    "that the given model 70% predicting data of unknown values,it is mainly used to simple linear regression,it cannot used \n",
    "for multiple dependent features\n",
    "R squred given by:\n",
    "    1-(SSres)/(SStotal)\n",
    "where:\n",
    "    SSres:sum of square resudials\n",
    "    SStotal: sum of square total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88260192-9098-4990-b799-ceb80a520d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47817302-1ffc-4ad9-84ef-d8d3b1b0ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted R-squared in linear regression models used for the performance matrix in model.it tells how the model is performed\n",
    "to the test data after the training of data with the model,suppose let us considerd if model peformance is 70%,it tells\n",
    "that the given model 70% predicting data of unknown values,it is mainly used to multiple linear regression,it can used \n",
    "for multiple dependent features\n",
    "\n",
    "adjusted R squred given by:\n",
    "    1-(1-r^2)(n-1)/(n-p-1)\n",
    "    where:\n",
    "        n:no of data points\n",
    "        p:no of independent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea8ce9-3bd5-485b-ad35-e28cf3a7ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "when we have more than one dependent features we use adjusted r-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70dee5a-ab3c-4a7c-9ed9-eee60208cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7f561-0ac4-4283-944f-7f38bc014077",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE is a mean squared error in regression analysis defined as average of square of actual outputs minus predicted outputs\n",
    "MSE calculated as:\n",
    "    mse=1/n[E(Yi-Y^)^2]\n",
    "MAE is a mean absolute error in regression analysis defined as average  of actual outputs minus predicted outputs\n",
    "MAE calculated as:\n",
    "    mae=1/n[E(Yi-Y^)]\n",
    "RMSE is a Root mean squared error in regression analysis defined as square root of average of square of actual outputs minus predicted outputs\n",
    "MSE calculated as:\n",
    "    rmse=sqrt(1/n [E(Yi-Y^)^2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce249e-8a29-4671-86a6-6beed669ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61a863-6f76-4b2d-a402-e3d039b77a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages of MSE:\n",
    "    it differentiable at every point in gradient descent curve\n",
    "    it has one local and global minima\n",
    "Disadvantages of MSE:\n",
    "    it does not robust to outliers\n",
    "    after applying the mse the units cost function changes with respect features\n",
    "advantages of MAE:\n",
    "    Robust to outliers\n",
    "    after applying the mae the units cost function same with respect features\n",
    "disadvantages of MAE:\n",
    "    convergence usually takes time\n",
    "    optimization is complex\n",
    "    Time complexity\n",
    "advantages of RMSE:\n",
    "    it differentiable at every point in gradient descent curve\n",
    "    after applying the rmse the units cost function same with respect features\n",
    "disadvantages of RMSE:\n",
    "    Not robust to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532880c1-f994-4892-aa38-b520f5a57a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061dba7-96b8-4d05-810b-c69b79f01d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso regularization also known as the l1 regularization mainly used for reduce overfitting and feature selection\n",
    "Lasso regularization mainly used for overfitting models and removes features that are not correlated in which we use \n",
    "extra hyperparametical and we use summation of slopes to the cost function,when we calculting the cost function at some \n",
    "point slope became zero which means the feature is not correlated so we remove that feature\n",
    "* In ridge regression there is no point slope became zero,so we cannot extract a feature that has no correlation but we can extract from \n",
    "lassoregression we use absolute slope that are equal to zero\n",
    "lasso regression mainly used during the high dimensionality dataset to remove the feature that are not corelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf9697-7312-418a-8be4-2d012fa95db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34743140-800f-4149-95e8-ba67d77bd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "we regularized model with l1 regularization and l2 regularization during the overfitting in model the model will have\n",
    "zero error during the training so that during training accuracy is heigh and which during testing of our model variance is low \n",
    "and predict the wrong output,so that we use regularization and we add penality term which is hyperparameter lambda and slope of\n",
    "the values so that model will not overfitt exactly,somehow we get errors in training and we get accurate results in testing of our model\n",
    "\n",
    "Suppose we have a dataset of houses with features like square footage, number of bedrooms, and age, and our task is to predict the sale prices of these houses. We will build a linear regression model to make these predictions.\n",
    "Without Regularization:\n",
    "We start by fitting a regular linear regression model without any regularization. The model tries to minimize the sum of squared errors between the predicted prices and the actual prices in the training data. The model may end up fitting the noise in the training data too closely, capturing specific patterns unique to the training set, and resulting in overfitting.\n",
    "\n",
    "With Lasso Regularization:\n",
    "To prevent overfitting, we can use Lasso regularization. Lasso adds a penalty term to the cost function, which is proportional to the sum of the absolute values of the coefficients:\n",
    "\n",
    "Î» is the regularization parameter that controls the strength of the regularization.\n",
    "\n",
    "Lasso encourages some of the coefficients to become exactly zero during the training process, effectively performing feature selection. This means that some features may be deemed irrelevant and have no impact on the final predictions. By removing such irrelevant features, Lasso helps the model focus on the most important ones, which can lead to a simpler and more generalized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ac01a-b9f9-48f8-91f5-92dcccdb5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d53d10-f249-4e31-bee5-a029f1769aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models, such as Ridge (L2 regularization) and Lasso (L1 regularization), offer valuable benefits in preventing overfitting and handling high-dimensional datasets. However, they do have certain limitations, and there are situations where they may not be the best choice for regression analysis. Let's discuss some of these limitations:\n",
    "\n",
    "Loss of interpretability: Regularization techniques like Lasso tend to set some coefficients exactly to zero, effectively performing feature selection. While this can simplify the model and improve generalization, it also makes the model less interpretable since some features are completely excluded from the final model. Interpreting the importance of specific features becomes challenging in such cases.\n",
    "\n",
    "Overshrinking coefficients: In situations where all features are relevant and contribute to the target variable, excessive regularization can lead to overshrinking of the coefficients. This means that the model may not fully capture the complexities and nuances of the data, resulting in reduced model performance.\n",
    "\n",
    "Hyperparameter tuning: Regularized linear models have hyperparameters (e.g., \n",
    "ï¿½\n",
    "Î» in Lasso and Ridge) that need to be tuned. Selecting the right values for these hyperparameters requires cross-validation or other optimization techniques. The process of hyperparameter tuning can be time-consuming and computationally expensive.\n",
    "\n",
    "Limited representation of complex relationships: Linear models, even with regularization, have inherent limitations when it comes to capturing complex nonlinear relationships between features and the target variable. In cases where the data exhibits intricate interactions and nonlinearity, more sophisticated models like decision trees, random forests, or neural networks might be more appropriate.\n",
    "\n",
    "Data preprocessing challenges: Regularized linear models are sensitive to the scale of features. In some cases, extensive feature scaling and normalization might be necessary to ensure the regularization term operates effectively, which adds complexity to the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f989395-fd59-41dd-a895-218dbceeac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6c596-13dc-4412-9c5c-14b5e6d0f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "In the scenario given, Model A has an RMSE (Root Mean Squared Error) of 10, while Model B has an MAE (Mean Absolute Error) of 8. To determine which model is the better performer, we need to understand the characteristics of each evaluation metric and their implications.\n",
    "\n",
    "RMSE takes the square root of the average of the squared differences between the predicted and actual values. It penalizes larger errors more heavily due to the squaring operation. RMSE is expressed in the same units as the target variable, which makes it easily interpretable.\n",
    "\n",
    "MAE, on the other hand, calculates the average absolute differences between the predicted and actual values. Unlike RMSE, it does not square the errors and treats all errors equally. MAE is also expressed in the same units as the target variable, making it interpretable as well.\n",
    "\n",
    "When comparing the two models, lower values for both RMSE and MAE indicate better predictive performance and accuracy. However, the difference in their absolute values can be misleading in deciding which model is better. In this case, Model B has a lower MAE of 8 compared to Model A's RMSE of 10.\n",
    "\n",
    "Considering the difference in the values of the metrics, Model B (MAE of 8) seems to perform better than Model A (RMSE of 10). This suggests that, on average, Model B has smaller errors between the predicted and actual values, which makes it a more accurate predictor.\n",
    "\n",
    "Limitations to the choice of metric:\n",
    "While MAE and RMSE are commonly used evaluation metrics, they have their limitations:\n",
    "\n",
    "Sensitivity to outliers: RMSE is more sensitive to outliers compared to MAE because it squares the errors. A few extreme outliers can disproportionately influence the RMSE value. In contrast, MAE is more robust to outliers since it only considers the absolute differences.\n",
    "\n",
    "Interpretability: While both metrics are interpretable in the same units as the target variable, RMSE may be less intuitive to understand for non-technical stakeholders due to the squaring operation.\n",
    "\n",
    "Magnitude of errors: RMSE gives more weight to large errors, which might not always be desirable in certain applications. For example, in some scenarios, we may want to prioritize minimizing the impact of larger errors on the final predictions.\n",
    "\n",
    "Impact of the dataset scale: Both metrics can be affected by the scale of the target variable. If the target variable has a large range of values, it may lead to larger RMSE and MAE values, making direct comparisons between models more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e3b43-0d5a-41c7-9f95-186ae42a4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3825aa-7786-4ae5-92be-9507089fa900",
   "metadata": {},
   "outputs": [],
   "source": [
    "To determine which model is the better performer, we need to consider the following:\n",
    "\n",
    "Interpretability: Ridge regularization tends to keep all features (coefficients) non-zero, while Lasso regularization can set some coefficients exactly to zero. If interpretability is crucial, Ridge may be preferred as it retains all features and their corresponding contributions to the predictions.\n",
    "\n",
    "Feature Selection: If the dataset contains many irrelevant or redundant features, Lasso regularization's ability to perform feature selection by setting some coefficients to zero may result in a more concise and interpretable model.\n",
    "\n",
    "Regularization Strength: The choice of regularization parameter (\n",
    "ï¿½\n",
    "Î») affects the level of regularization applied. A smaller \n",
    "ï¿½\n",
    "Î» value indicates weaker regularization, and a larger \n",
    "ï¿½\n",
    "Î» value indicates stronger regularization. The optimal value of \n",
    "ï¿½\n",
    "Î» for each model needs to be determined through cross-validation or other tuning techniques.\n",
    "\n",
    "Data Characteristics: The choice between Ridge and Lasso regularization also depends on the characteristics of the dataset. For instance, if there are strong correlations between predictors (multicollinearity), Lasso may be preferred, as it can effectively handle multicollinearity and potentially identify the dominant predictors.\n",
    "\n",
    "Trade-offs and Limitations:\n",
    "Both Ridge and Lasso regularization have their trade-offs and limitations:\n",
    "\n",
    "Ridge regularization may not perform feature selection as effectively as Lasso, and it can retain irrelevant features with reduced but non-zero coefficients.\n",
    "\n",
    "Lasso regularization may lead to a sparse model with some coefficients exactly set to zero, but this may come at the cost of losing information from those features, potentially reducing predictive accuracy.\n",
    "\n",
    "The choice of the regularization parameter \n",
    "ï¿½\n",
    "Î» needs to be carefully tuned. A poorly chosen value could lead to suboptimal model performance.\n",
    "\n",
    "Regularized linear models may not be suitable for capturing highly nonlinear relationships in the data. In such cases, more complex models like decision trees, random forests, or neural networks may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523f654-d901-4311-8efa-f880f16cb5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
