{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e400b14-65a7-43b6-94ba-1fb8afba7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dca5c0-d80d-4bbe-9a8c-1b0d5da45892",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple linear regresssion is mainly applicable to the one independent feature and one dependent feature\n",
    "multiple linear regression mainly applcable to the two or more independent feature and single dependent feature\n",
    "simple linear regression mainly applicable to 2d-plane\n",
    "multiple linear regression mainly applicable to 3d-plane\n",
    "simple linear regression example:\n",
    "    if we want to find the height of students based on the weight of the students we go for simple linear regression\n",
    "multiple linear regression example:\n",
    "    if we want to find the price of house based on the no of rooms,size of house we go for simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e83cbc-456c-4229-bef5-6cae22a01b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9fcbfa-32b1-4056-9f02-fd0d616672cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Linear regression relies on several assumptions to ensure the validity and reliability of the model. These assumptions are as follows:\n",
    "\n",
    "Linearity: The relationship between the dependent variable and independent variables is assumed to be linear. This means that the change in the dependent variable is directly proportional to the change in the independent variables. This assumption can be checked by creating scatter plots of the variables and visually inspecting if they exhibit a linear pattern.\n",
    "\n",
    "Independence: The observations in the dataset should be independent of each other. This assumption assumes that there is no correlation or relationship between the residuals (the differences between the observed and predicted values). This assumption can be assessed by examining the data collection methodology and ensuring that there are no dependencies or patterns in the residuals.\n",
    "\n",
    "Homoscedasticity: Homoscedasticity refers to the assumption that the variance of the residuals is constant across all levels of the independent variables. In other words, the spread of the residuals should be consistent throughout the range of the predictor variables. To check for homoscedasticity, you can plot the residuals against the predicted values and look for any patterns or systematic changes in the spread of the residuals.\n",
    "\n",
    "Normality: The residuals should follow a normal distribution, meaning they should be symmetrically distributed around a mean of zero. This assumption is important for performing hypothesis testing and constructing confidence intervals. You can assess normality by creating a histogram or a Q-Q plot of the residuals and checking if they closely resemble a normal distribution.\n",
    "\n",
    "No multicollinearity: Multicollinearity occurs when there is a high correlation between independent variables. It can cause instability in the parameter estimates and make it difficult to interpret the effects of individual predictors. To detect multicollinearity, you can calculate the correlation matrix among the independent variables and check for high correlation coefficients. Additionally, variance inflation factor (VIF) values can be computed, and if they are above a certain threshold (e.g., VIF > 5), it indicates multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f915a-d7a4-4c96-985e-f20485a27ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f07075-4d3d-4654-956e-1cf26209abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "we interprete the slope by what is the x coeffecients that are changes with the y coeffectient with one unit,we get\n",
    "positive slope when x increases and y also y increases,we get negative slope when x increases and y decreases\n",
    "we interprate the intercept is the predicted value according to dependent variable,it is the point where all independent \n",
    "values equal to zero,where intercept lies in y axis\n",
    "example:\n",
    "    Let us considerd salary of an employee as dependent variable and experience of an employee as independent variable\n",
    "    h0(x)=30000+5000*experence,here 30000 is the intercept where person with zero experience has 30000 intercept value\n",
    "    and 5000 is slope value in which for each additional experience is multiplied slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dd6b6-1974-4f9f-86dc-321a884634c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "gradient descent used as optimization algorithm used for the minimize the cost function or error iteratively adjusting\n",
    "model parameters each time to find out the global minima\n",
    "\n",
    "Here's a step-by-step explanation of the gradient descent process:\n",
    "\n",
    "Initialize Parameters: Start by initializing the model's parameters with random values or predefined values.\n",
    "\n",
    "Calculate the Cost Function: Evaluate the cost function using the current parameter values. The cost function quantifies the difference between the predicted and actual values and provides a measure of how well the model is performing.\n",
    "\n",
    "Compute the Gradient: Calculate the gradient of the cost function with respect to each parameter. The gradient indicates the direction and magnitude of the steepest ascent or descent. It helps determine the adjustments required to minimize the cost function.\n",
    "\n",
    "Update Parameters: Adjust the parameters by taking a step in the direction of the negative gradient (opposite to the direction of steepest ascent) multiplied by a learning rate. The learning rate determines the size of the steps taken during each iteration. It influences the convergence speed and stability of the algorithm.\n",
    "\n",
    "Repeat Steps 2-4: Iterate the process by recalculating the cost function, gradient, and updating the parameters. The process continues until a stopping criterion is met, such as reaching a maximum number of iterations or the cost function converging to a sufficiently low value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09aef4-faf1-49cc-b4e0-fdce38788559",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fb561-efb1-44ce-b2d7-4c23630e5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple linear regression mainly applcable to the two or more independent feature and single dependent feature\n",
    "multiple linear regression mainly applicable to 3d-plane\n",
    "multiple linear regression example:\n",
    "    if we want to find the price of house based on the no of rooms,size of house we go for simple linear regression\n",
    "simple linear regresssion is mainly applicable to the one independent feature and one dependent feature\n",
    "simple linear regression mainly applicable to 2d-plane\n",
    "simple linear regression example:\n",
    "    if we want to find the height of students based on the weight of the students we go for simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75709a3-1652-47e5-8abf-cc989c4cb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851630a-2960-448d-a60e-88de93087e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity refers to a situation in multiple linear regression where there is a high correlation or linear relationship between two or more independent variables. It can cause problems in the regression model, including unstable or unreliable coefficient estimates, difficulty in interpreting the effects of individual variables, and increased standard errors.\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "Correlation Matrix: Calculate the correlation matrix among the independent variables. High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): Compute the VIF for each independent variable. VIF measures how much the variance of the estimated coefficient is inflated due to multicollinearity. Higher VIF values (typically above 5 or 10) suggest the presence of multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Feature Selection: If multicollinearity is detected, consider removing one or more highly correlated variables from the regression model. By eliminating redundant variables, you can mitigate the impact of multicollinearity and improve the model's stability and interpretability.\n",
    "\n",
    "Data Collection: If multicollinearity persists, consider collecting additional data to reduce the correlation between variables. A larger and more diverse dataset can help alleviate multicollinearity issues.\n",
    "\n",
    "Ridge Regression or Lasso Regression: These regularization techniques can be used to handle multicollinearity. Ridge regression adds a penalty term to the regression objective function, which shrinks the coefficients towards zero and reduces the impact of multicollinearity. Lasso regression performs variable selection by setting some coefficients to zero, effectively eliminating variables with less importance.\n",
    "\n",
    "Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that transforms the original correlated variables into a new set of uncorrelated variables called principal components. By using the principal components as predictors, multicollinearity can be reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6e5a7-63b4-418d-a0f9-054cef41e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a247678-1696-4ac0-bc59-856d9222a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Polynomial regression is an extension of linear regression that allows for fitting curves or higher-degree polynomial functions to the data. While linear regression assumes a linear relationship between the dependent variable and the independent variables, polynomial regression can capture non-linear relationships by introducing polynomial terms.\n",
    "\n",
    "In linear regression, the relationship between the dependent variable (Y) and independent variable(s) (X) is modeled as a linear equation:\n",
    "\n",
    "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "Y is the dependent variable\n",
    "X₁, X₂, ..., Xₚ are the independent variables\n",
    "β₀, β₁, β₂, ..., βₚ are the regression coefficients (intercept and slopes)\n",
    "ε is the error term\n",
    "In polynomial regression, we introduce additional polynomial terms of the independent variable(s) to capture non-linear relationships. For example, a polynomial regression model of degree 2 can be represented as:\n",
    "\n",
    "Y = β₀ + β₁X₁ + β₂X₁² + β₃X₂ + β₄X₂² + ... + βₚXₚ + ε\n",
    "\n",
    "The higher-degree polynomial terms (X₁², X₂², etc.) allow the model to fit curves and capture non-linear patterns in the data. By increasing the degree of the polynomial, the model becomes more flexible and can capture more complex relationships between the variables.\n",
    "\n",
    "The main difference between linear regression and polynomial regression is the presence of higher-degree polynomial terms in the latter. Linear regression assumes a linear relationship, while polynomial regression can account for non-linearities. Polynomial regression can be useful when the relationship between the variables is expected to have curvature or when simple linear regression fails to capture the underlying pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d48cdb-34ca-403e-89a9-1ebd8d44d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "Situations where Polynomial Regression may be preferred:\n",
    "\n",
    "Non-Linear Relationships: When there is a clear expectation or evidence of a non-linear relationship between the dependent and independent variables, polynomial regression can be more appropriate. It allows for modeling curved or non-linear patterns in the data.\n",
    "\n",
    "Improved Model Fit: If linear regression fails to adequately capture the relationship between the variables and the residuals exhibit a clear non-linear pattern, polynomial regression can provide a better fit to the data.\n",
    "\n",
    "Feature Engineering: Polynomial regression can be useful in feature engineering, where new features are created by transforming the original features using polynomial terms. This approach can help uncover complex relationships and interactions between variables.\n",
    "\n",
    "Limited Extrapolation: If the goal is to make predictions within the observed range of the data and there is no need for reliable extrapolation, polynomial regression can be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e3626-29ea-4e46-8b91-bc0ad0c81a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
