{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65880064-c481-4003-aca4-6fef05bf9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17deb74-3b21-422e-87b3-65db4c7070ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Missing values in a dataset refer to the absence of a particular value or information in one or more variables of the dataset. They can occur due to various reasons, such as human errors during data collection, equipment malfunction, or incomplete responses from survey participants.\n",
    "\n",
    "Handling missing values is crucial for several reasons:\n",
    "\n",
    "Accurate analysis: Missing values can lead to biased or incorrect results if not handled properly. They can affect statistical measures, such as means, standard deviations, and correlations, thus impacting the validity of the analysis.\n",
    "\n",
    "Reliable modeling: Many machine learning algorithms cannot directly handle missing values and may produce errors or biased models if missing values are present. Therefore, handling missing values ensures the reliability and effectiveness of the models.\n",
    "\n",
    "Complete insights: Missing values can cause the loss of valuable information, reducing the comprehensiveness of the dataset. By handling missing values appropriately, researchers can gain a more complete understanding of the data and draw more meaningful insights.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "\n",
    "Decision trees: Decision tree-based algorithms, such as Random Forest and Gradient Boosting, can naturally handle missing values. They do not require imputation or removal of missing values since the tree splits can be based on available features in the dataset.\n",
    "\n",
    "Support Vector Machines (SVM): SVM algorithms can handle missing values by omitting the corresponding samples during the training process. As SVMs rely on support vectors for decision boundaries, missing values do not impact the performance as long as they are not part of the support vectors.\n",
    "\n",
    "Gaussian Mixture Models (GMM): GMM algorithms are robust to missing values, as they estimate the model parameters using the Expectation-Maximization (EM) algorithm. During the EM iterations, missing values are treated as latent variables, and the algorithm can still converge to appropriate solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55ca67-69e4-417e-9f6f-cb8a3b7cd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ef47a-c3cd-4348-ab52-6d5a795f0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques used to handle missing data are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db883805-c28d-48f9-8561-811481d28066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "4  5.0  10.0\n",
      "     A     B\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "#deletion\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'A':[None,3,None,4,5],'B':[6,None,8,None,10]})\n",
    "#list-wise deletion\n",
    "print(df.dropna())\n",
    "#pairwise deletion\n",
    "print(df[['A','B']].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d79da85-4b47-4fcb-acdb-d93327b85a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  4.0   6.0\n",
      "1  3.0   8.0\n",
      "2  4.0   8.0\n",
      "3  4.0   8.0\n",
      "4  5.0  10.0\n",
      "     A     B\n",
      "0  NaN   6.0\n",
      "1  3.0   6.0\n",
      "2  3.0   8.0\n",
      "3  4.0   8.0\n",
      "4  5.0  10.0\n",
      "     A     B\n",
      "0  3.0   6.0\n",
      "1  3.0   8.0\n",
      "2  4.0   8.0\n",
      "3  4.0  10.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "#mean value imputation\n",
    "df_mean=df.mean()\n",
    "print(df.fillna(df_mean))\n",
    "#forward fill ffill\n",
    "print(df.ffill())\n",
    "#backward fill bfill\n",
    "print(df.bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9d7293b-1e33-4159-a958-9a18223cf8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B\n",
       "0  NaN   6.0\n",
       "1  3.0   7.0\n",
       "2  3.5   8.0\n",
       "3  4.0   9.0\n",
       "4  5.0  10.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Interpolation\n",
    "df.interpolate('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db166a01-3648-4498-a06d-b7bc5491eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8d973-9f5b-488a-8119-eb2c6204e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalanced data refers to a situation in which the distribution of classes or target variables in a dataset is significantly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da305b-814e-474a-911d-ff75ca6473b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Biased Model: When a dataset is imbalanced, a machine learning model tends to be biased towards the majority class. It focuses more on correctly predicting the majority class, often resulting in poor performance for the minority class. The model may fail to capture the patterns and characteristics of the minority class, leading to low recall or sensitivity for the minority class.\n",
    "\n",
    "Incorrect Evaluation Metrics: Traditional evaluation metrics, such as accuracy, can be misleading in the case of imbalanced data. Even if a model predicts the majority class accurately, it may fail to identify instances from the minority class. Consequently, accuracy can be high while the model's effectiveness in detecting the minority class is extremely poor. Evaluation metrics like precision, recall, F1-score, and area under the ROC curve (AUC-ROC) are more appropriate for imbalanced datasets.\n",
    "\n",
    "Poor Generalization: Models trained on imbalanced data may struggle to generalize well to unseen data, especially if the test data has a different class distribution. This is because the model has not learned the underlying patterns of the minority class effectively during training.\n",
    "\n",
    "Decision Threshold Bias: The decision threshold of a classification model determines the class assignment based on predicted probabilities. Imbalanced data can result in an incorrect threshold selection, favoring the majority class. This can further exacerbate the issue of misclassifying instances from the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773e153-9ad7-41f4-aae3-3cbbe3f6ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85f58c-5748-47b4-b2ef-ef6f1d2237e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "up-sampling defined as when there is a imbalance dataset where there is majority \n",
    "class and minority class in which we have to increas the target values of minority values\n",
    "by the method of up sampling to balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb2e4d-d5d1-478b-aa9f-54f092ad7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "For example, in image processing, up-sampling can be used to increase the resolution \n",
    "of a low-resolution image. By inserting additional pixels between existing pixels, the \n",
    "image can be enhanced to a higher resolution, revealing more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2028c-4f3c-48e7-85b4-bfa98007468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "down-sampling defined as when there is a imbalance dataset where there is majority \n",
    "class and minority class in which we have to decreases the target values of majority values\n",
    "by the method of dowana sampling to balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1de8c1-edaa-46e5-b3fd-66d252a56705",
   "metadata": {},
   "outputs": [],
   "source": [
    "For example, in image processing, down-sampling can be used to reduce the resolution of a high-resolution\n",
    "image. By removing selected pixels, the image can be transformed into a lower resolution version, which may \n",
    "be more suitable for certain applications such as web display or thumbnail generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de65af2-816a-4ae6-9819-31587dc88f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4d172-c850-4930-91ad-ce69152e3fd6",
   "metadata": {},
   "source": [
    "outliers in dataset defined as Outliers in a dataset refer to data points that are significantly different from the majority of the other data points. These data points lie at an extreme distance from the central tendency of the data distribution and can have a disproportionate influence on statistical analysis, modeling, and machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc576ed-e1a3-4264-b19e-9f034b691ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "handling outliers is important to ensure accurate analysis, reliable modeling, and meaningful interpretation of data.\n",
    "The specific approach to handling outliers depends on the nature of the data, the context, and the goals of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2a7f1-0c8c-43d1-a477-8e8a972307a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af8062-e1e7-47f5-b7bf-1051fc505613",
   "metadata": {},
   "outputs": [],
   "source": [
    " some techniques you can use to handle the missing data in your analysis are:\n",
    "        *deletion of null values\n",
    "        *mean,median,mode imputation\n",
    "        *interpolation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc148dca-a077-4a08-bee4-946547fe4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3acf9-e2e8-443c-9750-f404b7805437",
   "metadata": {},
   "outputs": [],
   "source": [
    "Descriptive statistics: Calculate summary statistics (e.g., mean, median, standard deviation) for variables with missing data and compare them to those with complete data. If there are significant differences, it may indicate a pattern in the missingness.\n",
    "\n",
    "Missing data visualization: Create visualizations such as bar charts or heatmaps to visualize the missingness patterns across variables. This can help identify any visible patterns or dependencies between missing values.\n",
    "\n",
    "Missing data mechanism tests: Employ statistical tests to assess the missing data mechanism. Some commonly used tests include:\n",
    "\n",
    "Little's MCAR test: This test assesses whether the missingness is completely at random (MCAR). It tests the null hypothesis that the missingness is unrelated to the observed and unobserved data. If the p-value is not significant, it suggests that the missingness is MCAR.\n",
    "\n",
    "Chi-square test: This test can be used to evaluate the relationship between missingness and other variables. By comparing the observed and expected frequencies, it helps determine if the missingness is related to specific factors.\n",
    "\n",
    "Missingness pattern tests: These tests examine the relationship between the missingness of one variable and the values of other variables. They can provide insights into potential patterns or dependencies in the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da7164-a08b-49ce-a9da-f632e3644170",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d5f58-d6b8-410d-971c-018bdf471fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resampling techniques: Address the class imbalance by resampling the dataset. Some techniques include:\n",
    "\n",
    "Oversampling: Increase the number of instances in the minority class by replicating or generating synthetic examples. This can be done using methods like Random Oversampling, SMOTE (Synthetic Minority Over-sampling Technique), or ADASYN (Adaptive Synthetic Sampling).\n",
    "\n",
    "Undersampling: Decrease the number of instances in the majority class by randomly removing examples. This can be done using methods like Random Undersampling, Tomek Links, or NearMiss.\n",
    "\n",
    "Hybrid approaches: Combine oversampling and undersampling techniques to achieve a more balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36716c03-ccfd-459d-93e0-6d9cd63ea827",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b87b76-520b-4237-90e8-dd834757e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random under-sampling: Randomly remove instances from the majority class to match the number of instances in the minority class. This method reduces the dataset size but may also discard potentially useful information.\n",
    "\n",
    "Cluster-based under-sampling: Use clustering algorithms to identify clusters within the majority class and then randomly sample instances from each cluster. This approach aims to preserve the diversity of the majority class while reducing its size.\n",
    "\n",
    "Tomek Links: Identify pairs of instances from different classes that are close to each other but of opposite classes. Remove the majority class instances from these pairs, as they are considered more likely to be misclassified.\n",
    "\n",
    "Edited Nearest Neighbors (ENN): Apply a nearest neighbor algorithm to identify majority class instances that are misclassified. Remove those instances from the dataset to balance the classes.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): Generate synthetic instances of the minority class by interpolating between existing minority class instances. This technique helps increase the representation of the minority class without losing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e1780-0b4a-4080-8aec-3c83f7f4b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeeb7f6-9a4c-4922-aa6e-f8439ea351cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random over-sampling: Randomly duplicate instances from the minority class to increase its size and match the number of instances in the majority class. This method may lead to overfitting if the duplicated instances introduce too much redundancy.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): Generate synthetic instances of the minority class by interpolating between existing minority class instances. This technique helps increase the representation of the minority class while introducing less redundancy compared to random over-sampling.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN): Similar to SMOTE, ADASYN generates synthetic instances of the minority class. However, it places more emphasis on difficult-to-learn instances, making it suitable for imbalanced datasets with complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906db02d-a894-40f7-bf01-f31cc1ee18f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856b53d-7f14-47b3-965f-4c744c8860e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c4672-50fa-4d11-ba48-d5356cc28aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be88638-6a31-4a0e-9274-9e8833af39ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83f506-e671-4760-9828-43099f2462fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7fbd3-1328-485e-9e09-ce251bf26d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429374b-4d8b-481e-b13a-a383607a1193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5cc25-4a56-4fcf-80bd-e702cad9997e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf221f3a-5b9d-41b4-947b-dd06af6c1d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6548be-4b1d-485c-8b15-fd48b0825f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7bb40-0ac4-469b-9b0e-a0d4a569d8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c4023-1c94-40a4-8a8c-d06d7526846b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b8a1e-d5f5-401b-8f6b-2e2e8829d839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d2994-a7bb-459d-9764-91bb300c69b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
